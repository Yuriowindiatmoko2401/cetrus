"""Utility functions for DATSR Streamlit app"""

import os
import sys
import time
import streamlit as st
import torch
import numpy as np


def setup_page_config():
    """Configure Streamlit page settings"""
    st.set_page_config(
        page_title="DATSR Super-Resolution",
        page_icon="ğŸ–¼ï¸",
        layout="wide",
        initial_sidebar_state="expanded"
    )


def add_custom_css():
    """Add custom CSS styling"""
    st.markdown("""
    <style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        text-align: center;
        color: #1f77b4;
        margin-bottom: 1rem;
    }

    .metric-container {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
    }

    .upload-container {
        border: 2px dashed #ddd;
        border-radius: 0.5rem;
        padding: 2rem;
        text-align: center;
        margin: 1rem 0;
    }

    .success-message {
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
        color: #155724;
        padding: 1rem;
        border-radius: 0.25rem;
        margin: 1rem 0;
    }

    .error-message {
        background-color: #f8d7da;
        border: 1px solid #f5c6cb;
        color: #721c24;
        padding: 1rem;
        border-radius: 0.25rem;
        margin: 1rem 0;
    }

    .info-message {
        background-color: #d1ecf1;
        border: 1px solid #bee5eb;
        color: #0c5460;
        padding: 1rem;
        border-radius: 0.25rem;
        margin: 1rem 0;
    }

    /* Hide Streamlit footer */
    footer {
        visibility: hidden;
    }

    /* Custom button styling */
    .stButton > button {
        background-color: #1f77b4;
        color: white;
        border: none;
        padding: 0.5rem 1rem;
        border-radius: 0.25rem;
        font-weight: bold;
    }

    .stButton > button:hover {
        background-color: #155a8a;
    }
    </style>
    """, unsafe_allow_html=True)


def check_dependencies():
    """Check if all required dependencies are available"""
    missing_deps = []

    try:
        import torch
        import torchvision
    except ImportError:
        missing_deps.append("PyTorch")

    try:
        import cv2
    except ImportError:
        missing_deps.append("OpenCV")

    try:
        import mmcv
    except ImportError:
        missing_deps.append("mmcv-full")

    try:
        import PIL
    except ImportError:
        missing_deps.append("Pillow")

    try:
        import numpy
    except ImportError:
        missing_deps.append("NumPy")

    return missing_deps


def check_cuda_availability():
    """Check CUDA availability and provide recommendations"""
    if torch.cuda.is_available():
        device_name = torch.cuda.get_device_name(0)
        memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)

        return {
            'available': True,
            'device_name': device_name,
            'memory_gb': memory_gb,
            'recommendation': f"GPUåŠ é€Ÿå¯ç”¨ - {device_name} ({memory_gb:.1f}GB)"
        }
    else:
        return {
            'available': False,
            'recommendation': "ä½¿ç”¨CPUå¤„ç† - å»ºè®®å®‰è£…CUDAç‰ˆæœ¬ä»¥è·å¾—æ›´å¿«é€Ÿåº¦"
        }


def validate_model_files():
    """Check if pretrained model files exist"""
    base_path = os.path.join(os.path.dirname(__file__), '..', '..', 'DATSR', 'experiments', 'pretrained_model')

    required_files = [
        'feature_extraction.pth',
        'restoration_mse.pth',
        'restoration_gan.pth'
    ]

    missing_files = []
    existing_files = []

    for file in required_files:
        file_path = os.path.join(base_path, file)
        if os.path.exists(file_path):
            size_mb = os.path.getsize(file_path) / (1024 * 1024)
            existing_files.append(f"âœ… {file} ({size_mb:.1f}MB)")
        else:
            missing_files.append(f"âŒ {file}")

    return {
        'all_exist': len(missing_files) == 0,
        'existing_files': existing_files,
        'missing_files': missing_files,
        'model_path': base_path
    }


def render_system_status():
    """Render system status dashboard"""
    with st.expander("ğŸ”§ ç³»ç»ŸçŠ¶æ€æ£€æŸ¥", expanded=False):
        # Check dependencies
        missing_deps = check_dependencies()

        st.markdown("### ğŸ“¦ ä¾èµ–æ£€æŸ¥")
        if not missing_deps:
            st.success("âœ… æ‰€æœ‰ä¾èµ–éƒ½å·²æ­£ç¡®å®‰è£…")
        else:
            st.error("âŒ ç¼ºå°‘ä»¥ä¸‹ä¾èµ–:")
            for dep in missing_deps:
                st.error(f"â€¢ {dep}")

        # Check CUDA
        st.markdown("### ğŸš€ GPUçŠ¶æ€")
        cuda_info = check_cuda_availability()
        if cuda_info['available']:
            st.success(cuda_info['recommendation'])
            st.info(f"æ˜¾å­˜: {cuda_info['memory_gb']:.1f}GB")
        else:
            st.warning(cuda_info['recommendation'])

        # Check model files
        st.markdown("### ğŸ¤– æ¨¡å‹æ–‡ä»¶æ£€æŸ¥")
        model_status = validate_model_files()

        if model_status['all_exist']:
            st.success("âœ… æ‰€æœ‰é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶éƒ½å­˜åœ¨")
        else:
            st.error("âŒ ç¼ºå°‘æ¨¡å‹æ–‡ä»¶:")
            for missing in model_status['missing_files']:
                st.error(f"{missing}")

            st.info(f"ğŸ“ æ¨¡å‹è·¯å¾„: {model_status['model_path']}")
            st.markdown("""
            **ä¸‹è½½è¯´æ˜:**
            è¯·è®¿é—® [DATSR GitHub Releases](https://github.com/caojiezhang/DATSR/releases) ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹,
            å¹¶å°†å®ƒä»¬æ”¾ç½®åœ¨ `DATSR/experiments/pretrained_model/` ç›®å½•ä¸­ã€‚
            """)


def create_progress_callback():
    """Create a progress callback for Streamlit"""
    progress_bar = st.progress(0)
    status_text = st.empty()

    def callback(message, progress):
        progress_bar.progress(progress)
        status_text.text(message)

    return callback, progress_bar, status_text


def clear_progress(progress_bar, status_text):
    """Clear progress indicators"""
    progress_bar.empty()
    status_text.empty()


def format_time(seconds):
    """Format time in seconds to human readable format"""
    if seconds < 60:
        return f"{seconds:.2f}s"
    elif seconds < 3600:
        minutes = int(seconds // 60)
        remaining_seconds = seconds % 60
        return f"{minutes}m {remaining_seconds:.1f}s"
    else:
        hours = int(seconds // 3600)
        remaining_minutes = int((seconds % 3600) // 60)
        return f"{hours}h {remaining_minutes}m"


def estimate_processing_time(image_shape, scale_factor, device='cpu'):
    """Estimate processing time based on image size and device"""
    height, width = image_shape[:2]
    pixels = height * width

    # Base processing times (in seconds)
    base_time_cpu = pixels / (1000 * 1000) * 0.5  # 0.5s per megapixel on CPU
    base_time_gpu = pixels / (1000 * 1000) * 0.05  # 0.05s per megapixel on GPU

    # Scale factor adjustment
    scale_adjustment = scale_factor / 4.0  # Adjust based on scale factor

    if device == 'cuda':
        estimated_time = base_time_gpu * scale_adjustment
    else:
        estimated_time = base_time_cpu * scale_adjustment

    return estimated_time


def save_to_session_state(key, value):
    """Save value to Streamlit session state"""
    st.session_state[key] = value


def load_from_session_state(key, default=None):
    """Load value from Streamlit session state"""
    return st.session_state.get(key, default)


def clear_session_state(keys=None):
    """Clear specific keys or all session state"""
    if keys is None:
        st.session_state.clear()
    else:
        for key in keys:
            if key in st.session_state:
                del st.session_state[key]


class ErrorHandler:
    """Handle and display errors gracefully"""

    @staticmethod
    def handle_processing_error(error, show_traceback=False):
        """Handle processing errors with user-friendly messages"""
        error_str = str(error)

        # Common error patterns
        if "CUDA out of memory" in error_str:
            st.error("ğŸš« GPUå†…å­˜ä¸è¶³! è¯·å°è¯•:")
            st.error("â€¢ ä½¿ç”¨è¾ƒå°çš„å›¾ç‰‡")
            st.error("â€¢ åˆ‡æ¢åˆ°CPUæ¨¡å¼")
            st.error("â€¢ é‡å¯åº”ç”¨é‡Šæ”¾å†…å­˜")

        elif "model" in error_str.lower() and "not found" in error_str.lower():
            st.error("ğŸ¤– æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°!")
            st.error("è¯·æ£€æŸ¥é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶æ˜¯å¦æ­£ç¡®ä¸‹è½½å¹¶æ”¾ç½®åœ¨æŒ‡å®šç›®å½•")

        elif "Failed to load" in error_str:
            st.error("ğŸ“ æ–‡ä»¶åŠ è½½å¤±è´¥!")
            st.error("è¯·æ£€æŸ¥ä¸Šä¼ çš„å›¾ç‰‡æ ¼å¼æ˜¯å¦æ­£ç¡®")

        else:
            st.error(f"âŒ å¤„ç†å¤±è´¥: {error_str}")

        if show_traceback:
            with st.expander("æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯"):
                st.code(error_str)

    @staticmethod
    def handle_validation_error(validation_result):
        """Handle file validation errors"""
        is_valid, errors = validation_result

        if not is_valid:
            st.error("âŒ æ–‡ä»¶éªŒè¯å¤±è´¥:")
            for error in errors:
                st.error(f"â€¢ {error}")
            return False

        return True


def create_download_filename(prefix="datsr_result", extension="png"):
    """Create a unique filename for downloads"""
    import datetime
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"{prefix}_{timestamp}.{extension}"